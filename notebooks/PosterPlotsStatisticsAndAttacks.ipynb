{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5036a07-0faf-468a-8b87-fa8b6a355fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from pathlib import Path\n",
    "from context import LocalLearning\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "from scipy import stats\n",
    "\n",
    "import pickle\n",
    "\n",
    "plt.style.use(['seaborn-paper', \"./A1PosterPortrait.mplstyle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb19b83c-efd0-44e0-bbfa-200b1c0702bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "BATCH_SIZE = 1000\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc113c9-4bf2-487f-8b9e-efe10d4eb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "ll_model_path = Path(\"../data/models/CIFAR10_PowerLaw\")\n",
    "model_path = ll_model_path\n",
    "figure_path = Path(\"../data/figures/NORAConf23Poster\")\n",
    "\n",
    "# define file names\n",
    "khmodel_file = Path(\"KHModel.pty\")\n",
    "khmodel_scheduled_file = Path(\"KHModel_scheduled.pty\")\n",
    "bpmodel_file = Path(\"BPModel.pty\")\n",
    "bpmodel_scheduled_file = Path(\"BPModel_scheduled.pty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b66f68-3c20-4676-88ef-cda4c485bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15e676b-b99c-49cb-bf06-6023d0cc7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define colormap for all the poster plots\n",
    "cmap = {\n",
    "    \"kh\": \"#762a83\",\n",
    "    \"hybrid\": \"#f8a953\",\n",
    "    \"bp\": \"#1b7837\",#\"#106151\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b170e1a3-1a54-41b5-b180-f91637a585fa",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e02d1ad-192d-43ea-b745-db71bcfc632d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KHModel(\n",
       "  (local_learning): FKHL3(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (relu_h): ReLU()\n",
       "  (dense): Linear(in_features=2000, out_features=10, bias=True)\n",
       "  (softMax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "khmodel_state = torch.load(model_path / khmodel_file)\n",
    "khmodel = LocalLearning.KHModel(khmodel_state[\"fkhl3-state\"])\n",
    "khmodel.load_state_dict(khmodel_state[\"model_state_dict\"])\n",
    "khmodel.to(device)\n",
    "khmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4dc198-b1e9-4fe7-8d60-1a41f376e62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SHLP(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (ReLU): ReLU()\n",
       "  (dense): Linear(in_features=2000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpmodel_state = torch.load(model_path / bpmodel_file)\n",
    "bpmodel = LocalLearning.SHLP(params=bpmodel_state[\"params\"])\n",
    "bpmodel.load_state_dict(bpmodel_state[\"model_state_dict\"])\n",
    "bpmodel.to(device)\n",
    "bpmodel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8557519-e0a8-4d11-a10b-82c8247d5902",
   "metadata": {},
   "source": [
    "# Perform Mia's Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74971e80-a999-4ba7-865f-0aafab1b3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(outputs, targets):\n",
    "    # computationally stable\n",
    "    log_probs = nn.functional.log_softmax(outputs, dim=-1)\n",
    "    loss = (-log_probs.gather(1, targets[..., None])).sum() / len(outputs)\n",
    "    \n",
    "    # Compute negative log likelihood loss\n",
    "    #loss = nn.functional.nll_loss(log_probs, targets)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "ce_loss = cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8baddbc3-a9a1-4107-aefd-be6e204ecce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10Test= LocalLearning.LpUnitCIFAR10(\n",
    "            root=\"../data/CIFAR10\",\n",
    "            train=False,\n",
    "            transform=ToTensor(),\n",
    "            p=3.0,\n",
    "        )\n",
    "\n",
    "TestLoader = LocalLearning.DeviceDataLoader(\n",
    "            cifar10Test,\n",
    "            device=device,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=4,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a025017-d591-4d11-ab63-4a15cca7ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_critirium(\n",
    "    dataloader,\n",
    "    model, \n",
    "    crit,\n",
    "    thres = None\n",
    "    ):\n",
    "    \n",
    "    # Returns the data and the corresponding labels that meets the critirium given (crit)\n",
    "    \n",
    "    freq_correct = 0\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    \n",
    "    data = torch.zeros((0,32,32,3)).to(device)\n",
    "    lab_data = torch.zeros((0)).to(device)\n",
    "    \n",
    "    for batch_no, (features, labels) in enumerate(dataloader):\n",
    "        preds = model(features)\n",
    "        pred = torch.argmax(preds, dim=-1)\n",
    "        \n",
    "        if crit == \"correct\":\n",
    "            filtr_idx = (torch.abs(pred - labels) == 0)\n",
    "            new_features = features[filtr_idx]\n",
    "            data = torch.cat((data, new_features),dim=0)\n",
    "            new_labels = labels[filtr_idx]\n",
    "            lab_data = torch.cat((lab_data, new_labels),dim=0)\n",
    "        \n",
    "        elif crit == \"correct_thres\":\n",
    "            softmax_correct = (preds[torch.arange(1000),pred])\n",
    "            thres_idx = (softmax_correct >= thres)\n",
    "            correct_idx = (torch.abs(pred - labels) == 0)\n",
    "            filtr_idx = thres_idx & correct_idx \n",
    "            new_features = features[filtr_idx]\n",
    "            data = torch.cat((data, new_features),dim=0)\n",
    "            new_labels = labels[filtr_idx]\n",
    "            lab_data = torch.cat((lab_data, new_labels),dim=0)\n",
    "            \n",
    "        elif crit == \"thres\":\n",
    "            softmax_correct = (preds[torch.arange(1000),pred])\n",
    "            filtr_idx = (softmax_correct >= thres)\n",
    "            new_features = features[filtr_idx]\n",
    "            data = torch.cat((data, new_features),dim=0)\n",
    "            new_labels = labels[filtr_idx]\n",
    "            lab_data = torch.cat((lab_data, new_labels),dim=0)\n",
    "            \n",
    "        else: \n",
    "            raise ValueError(\"Not a valid criterium\")\n",
    "    \n",
    "    return data, lab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77306027-56f4-4743-ae1f-6acf06e90655",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_steps = 10000 \n",
    "step_size = 0.0001 \n",
    "eps_start = 0\n",
    "\n",
    "eps_list = [eps_start + n * step_size for n in range(max_num_steps + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af67f35-78ba-48d9-9623-0225f8610a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crit_eps(criterium, model, attack, print_accuracy=False):\n",
    "    \n",
    "    features, labels = data_critirium(TestLoader, model, criterium)\n",
    "    labels = labels.type(torch.LongTensor).to(device)\n",
    "    features.requires_grad = True\n",
    "    perturbed_image = features \n",
    "    \n",
    "    siz = len(labels)\n",
    "\n",
    "    crit_eps_per_image = torch.ones(siz).to(device).fill_(math.nan)   \n",
    "    crit_dist_per_image = torch.ones(siz).to(device).fill_(math.nan)\n",
    "\n",
    "    freq_correct = 0\n",
    "    total = 0 \n",
    "\n",
    "    loss_fn = ce_loss \n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    correct = []\n",
    "    b_norm = 0.05\n",
    "    \n",
    "    noise = torch.randn(features.shape).to(device)\n",
    "    \n",
    "    images = []\n",
    "    accuracy_dict = {}\n",
    "    accuracy_dict_actual = {}\n",
    "    \n",
    "    preds = model(features)\n",
    "    loss = loss_fn(preds, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    with tqdm(total=len(eps_list)) as pbar:\n",
    "        for i, eps in enumerate(eps_list):\n",
    "        \n",
    "            with torch.no_grad():\n",
    "\n",
    "                if attack == \"WN\":\n",
    "                    adv_image = features + eps*noise\n",
    "                    perturbed_image = torch.clamp(adv_image, min = 0, max = 1)\n",
    "\n",
    "                elif attack == \"PGD\":\n",
    "\n",
    "                    adv_image = perturbed_image + eps*features.grad.data.sign()\n",
    "                    clamp = torch.clamp(adv_image - features, min = -b_norm, max = b_norm)\n",
    "                    perturbed_image = torch.clamp(features + clamp, min = 0, max = 1)\n",
    "\n",
    "                elif attack == \"FGSM\":\n",
    "\n",
    "                    perturbed_image = features + eps*features.grad.data.sign()\n",
    "                    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "                preds_perturbed = torch.argmax(model(perturbed_image), dim=-1)\n",
    "                alike = (preds_perturbed == labels)\n",
    "                freq_correct = (torch.abs(preds_perturbed - labels) == 0).sum()\n",
    "                total = len(labels)\n",
    "                \n",
    "                accuracy = (freq_correct/total).item()\n",
    "                correct.append(accuracy)\n",
    "                \n",
    "                mask = (alike == False) & (crit_eps_per_image.isnan())\n",
    "\n",
    "                x = features.view(features.size(0),-1)\n",
    "                y = perturbed_image.view(perturbed_image.size(0),-1)\n",
    "\n",
    "                dist = torch.norm(x - y, dim=1).detach()\n",
    "\n",
    "                crit_dist_per_image[mask] = dist[mask]\n",
    "\n",
    "                perturbation = torch.abs(features - perturbed_image).detach()\n",
    "                avg_perturbation = torch.sum(perturbation.view(perturbation.size(0), -1), dim=1) / (perturbation.size(1) * perturbation.size(2) * perturbation.size(3))\n",
    "                crit_eps_per_image[mask] = avg_perturbation[mask]\n",
    "                \n",
    "                accuracy_dict_actual[eps] = accuracy*100\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                if (i % 2 == 0) and i <= 50:\n",
    "                    info = [perturbed_image[:10], labels[:10], preds_perturbed[:10], features, eps]\n",
    "                    images.append(info)\n",
    "\n",
    "    unique_crit_eps = torch.unique(crit_eps_per_image)\n",
    "    for unique_eps in unique_crit_eps:\n",
    "        if not math.isnan(unique_eps.item()):\n",
    "            not_misclassified = (torch.sum(crit_eps_per_image > unique_eps).item()*100)/siz\n",
    "            accuracy_dict[unique_eps.item()] = not_misclassified\n",
    "    \n",
    "    if print_accuracy == True:\n",
    "        print(f\"{correct[-1]*100:.2f}% is still correctly classified\")\n",
    "        print(f\"{100*(torch.sum(crit_eps_per_image.isnan()).item())/siz}% have been correctly classified at every step\")\n",
    "        \n",
    "    crit_eps_per_image = np.array(crit_eps_per_image.cpu())\n",
    "    crit_dist_per_image = np.array(crit_dist_per_image.cpu())\n",
    "    correct = np.array(correct)\n",
    "        \n",
    "    return crit_eps_per_image, crit_dist_per_image, correct, images, accuracy_dict, accuracy_dict_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f203c8c-624f-42df-a369-2a9d0966327a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82aa24bb4ddb4ac2a1358b34307a57b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.99% is still correctly classified\n",
      "7.122093023255814% have been correctly classified at every step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac529ec975b4973b5e65f7d5f4ee77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.80% is still correctly classified\n",
      "7.442748091603053% have been correctly classified at every step\n"
     ]
    }
   ],
   "source": [
    "criteps_bp_wn, critdist_bp_wn, correct_bp_wn, images_bp_wn, accuracy_dict_bp_wn, accuracy_act_bp_wn = crit_eps(\n",
    "    \"correct\", \n",
    "    bpmodel, \n",
    "    \"WN\", \n",
    "    print_accuracy=True,\n",
    ")\n",
    "criteps_ll_wn, critdist_ll_wn, correct_ll_wn, images_ll_wn, accuracy_dict_ll_wn, accuracy_act_ll_wn = crit_eps(\n",
    "    \"correct\", \n",
    "    khmodel, \n",
    "    \"WN\", \n",
    "    print_accuracy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3b7e1e0-8e03-465b-994c-21289632996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "khmodel_adversarial_data_WN = {\n",
    "    \"accuracy dict\": accuracy_dict_ll_wn, \n",
    "     \"actual accuracy dict\": accuracy_act_ll_wn,\n",
    "    \"critical epsilon\": criteps_ll_wn, \n",
    "    \"critical distance\": critdist_ll_wn, \n",
    "}\n",
    "bpmodel_adversarial_data_WN = {\n",
    "    \"accuracy dict\": accuracy_dict_bp_wn, \n",
    "     \"actual accuracy dict\": accuracy_act_bp_wn,\n",
    "    \"critical epsilon\": criteps_bp_wn, \n",
    "    \"critical distance\": critdist_bp_wn, \n",
    "}\n",
    "\n",
    "with open(model_path / Path(\"khmodel_attack_statistics_WhiteNoise.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(khmodel_adversarial_data_WN, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(model_path / Path(\"bpmodel_attack_statistics_WhiteNoise.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(bpmodel_adversarial_data_WN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7df36d66-2174-4656-a68e-211b7784fc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c725ba4ccd2645d3995643572f4442eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00% is still correctly classified\n",
      "0.0% have been correctly classified at every step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888a6dd1447f4f91af5328eac0a1e29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42% is still correctly classified\n",
      "0.0% have been correctly classified at every step\n"
     ]
    }
   ],
   "source": [
    "criteps_bp_fgsm, critdist_bp_fgsm, correct_bp_fgsm, images_bp_fgsm, accuracy_dict_bp_fgsm, accuracy_act_bp_fgsm = crit_eps(\n",
    "    \"correct\", \n",
    "    bpmodel, \n",
    "    \"FGSM\", \n",
    "    print_accuracy=True,\n",
    ")\n",
    "criteps_ll_fgsm, critdist_ll_fgsm, correct_ll_fgsm, images_ll_fgsm, accuracy_dict_ll_fgsm, accuracy_act_ll_fgsm = crit_eps(\n",
    "    \"correct\", \n",
    "    khmodel, \n",
    "    \"FGSM\", \n",
    "    print_accuracy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "127fd0c5-76cf-4c10-9b94-c580d7986bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "khmodel_adversarial_data_FGSM = {\n",
    "    \"accuracy dict\": accuracy_dict_ll_fgsm, \n",
    "     \"actual accuracy dict\": accuracy_act_ll_fgsm,\n",
    "    \"critical epsilon\": criteps_ll_fgsm, \n",
    "    \"critical distance\": critdist_ll_fgsm, \n",
    "}\n",
    "bpmodel_adversarial_data_FGSM = {\n",
    "    \"accuracy dict\": accuracy_dict_bp_fgsm, \n",
    "     \"actual accuracy dict\": accuracy_act_bp_fgsm,\n",
    "    \"critical epsilon\": criteps_bp_fgsm, \n",
    "    \"critical distance\": critdist_bp_fgsm, \n",
    "}\n",
    "\n",
    "with open(model_path / Path(\"khmodel_attack_statistics_FGSM.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(khmodel_adversarial_data_FGSM, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(model_path / Path(\"bpmodel_attack_statistics_FGSM.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(bpmodel_adversarial_data_FGSM, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458840a4-6623-43dd-bd7c-7632ef725c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789774192b524b6bb0368a3a683e6482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00% is still correctly classified\n",
      "0.0% have been correctly classified at every step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6f8bf05dda498481506effb0e41f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57% is still correctly classified\n",
      "0.4961832061068702% have been correctly classified at every step\n"
     ]
    }
   ],
   "source": [
    "criteps_bp_pgd, critdist_bp_pgd, correct_bp_pgd, images_bp_pgd, accuracy_dict_bp_pgd, accuracy_act_bp_pgd = crit_eps(\n",
    "        \"correct\", \n",
    "        bpmodel, \n",
    "        \"PGD\", \n",
    "        print_accuracy=True,\n",
    ")\n",
    "criteps_ll_pgd, critdist_ll_pgd, correct_ll_pgd, images_ll_pgd, accuracy_dict_ll_pgd, accuracy_act_ll_pgd = crit_eps(\n",
    "    \"correct\", \n",
    "    khmodel, \n",
    "    \"PGD\", \n",
    "    print_accuracy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0d8aceb-1c1e-4e39-b132-c01ae156e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "khmodel_adversarial_data_PGD = {\n",
    "    \"accuracy dict\": accuracy_dict_ll_pgd, \n",
    "     \"actual accuracy dict\": accuracy_act_ll_pgd,\n",
    "    \"critical epsilon\": criteps_ll_pgd, \n",
    "    \"critical distance\": critdist_ll_pgd, \n",
    "}\n",
    "bpmodel_adversarial_data_PGD = {\n",
    "    \"accuracy dict\": accuracy_dict_bp_pgd, \n",
    "     \"actual accuracy dict\": accuracy_act_bp_pgd,\n",
    "    \"critical epsilon\": criteps_bp_pgd, \n",
    "    \"critical distance\": critdist_bp_pgd, \n",
    "}\n",
    "\n",
    "with open(model_path / Path(\"khmodel_attack_statistics_PGD.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(khmodel_adversarial_data_PGD, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(model_path / Path(\"bpmodel_attack_statistics_PGD.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(bpmodel_adversarial_data_PGD, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f83a225-a0bd-41b4-99dc-b88315ca766b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dcec9e-1044-464c-990e-24cb349a9f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
